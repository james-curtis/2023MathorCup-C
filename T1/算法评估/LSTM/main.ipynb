{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:10.984703Z",
     "end_time": "2023-04-16T14:55:11.000699Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\win\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import *\n",
    "from statsmodels.graphics.tsaplots import *\n",
    "from catboost import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import *\n",
    "from sklearn.ensemble import *\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.metrics import *\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import *\n",
    "import os\n",
    "\n",
    "# 敏感性分析需要禁用v2_behavior\n",
    "# tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "forceCpu = False\n",
    "# 使用CPU\n",
    "if forceCpu:\n",
    "    cpu = tf.config.list_physical_devices(\"CPU\")\n",
    "    tf.config.set_visible_devices(cpu)\n",
    "    print(tf.config.list_logical_devices())\n",
    "\n",
    "# 动态显存\n",
    "if not forceCpu:\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    except:\n",
    "        print('Invalid device or cannot modify virtual devices once initialized')\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "palette = 'deep'\n",
    "sns.set(font='SimHei', font_scale=1.2, style='whitegrid', palette=palette)  # 解决Seaborn中文显示问题"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:11.004699Z",
     "end_time": "2023-04-16T14:55:14.362408Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "    场地1   场地2          日期    货量\n0   DC1   DC8  2021-01-01     3\n1  DC10  DC12  2021-01-01   306\n2  DC10  DC13  2021-01-01     4\n3  DC10  DC14  2021-01-01  2613\n4  DC10  DC17  2021-01-01     5",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>场地1</th>\n      <th>场地2</th>\n      <th>日期</th>\n      <th>货量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>DC1</td>\n      <td>DC8</td>\n      <td>2021-01-01</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>DC10</td>\n      <td>DC12</td>\n      <td>2021-01-01</td>\n      <td>306</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DC10</td>\n      <td>DC13</td>\n      <td>2021-01-01</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DC10</td>\n      <td>DC14</td>\n      <td>2021-01-01</td>\n      <td>2613</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>DC10</td>\n      <td>DC17</td>\n      <td>2021-01-01</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = pd.read_excel('../../../preprocess/左侧填充-物流网络历史货量数据.xlsx')\n",
    "rawData = pd.melt(rawData, id_vars=['场地1', '场地2'], value_name='货量', var_name='日期')\n",
    "rawData.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:14.362408Z",
     "end_time": "2023-04-16T14:55:22.208353Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总行数有：(765770, 4)\n",
      "数据小于33的行数有：(0, 5)\n",
      "清洗后数量：(765770, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   场地1  场地2  日期        货量\n0    1    8   0  1.098612\n1   10   12   0  5.723585\n2   10   13   0  1.386294\n3   10   14   0  7.868254\n4   10   17   0  1.609438",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>场地1</th>\n      <th>场地2</th>\n      <th>日期</th>\n      <th>货量</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1.098612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>12</td>\n      <td>0</td>\n      <td>5.723585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n      <td>13</td>\n      <td>0</td>\n      <td>1.386294</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10</td>\n      <td>14</td>\n      <td>0</td>\n      <td>7.868254</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10</td>\n      <td>17</td>\n      <td>0</td>\n      <td>1.609438</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "filterSize = 33\n",
    "\n",
    "\n",
    "def getxy():\n",
    "    targetKey = '货量'\n",
    "    # 对场地进行编码\n",
    "\n",
    "    df = rawData.copy()\n",
    "    # df = df[(df['场地1'] == 'DC14') & (df['场地2'] == 'DC10')].reset_index(drop=True)\n",
    "    df['场地1'] = df['场地1'].str.replace('DC', '')\n",
    "    df['场地1'] = df['场地1'].astype('int64')\n",
    "    df['场地2'] = df['场地2'].str.replace('DC', '')\n",
    "    df['场地2'] = df['场地2'].astype('int64')\n",
    "\n",
    "    df['日期'] = pd.to_datetime(df['日期'])\n",
    "    df['日期'] = df['日期'] - df['日期'].min()\n",
    "    df['日期'] = df['日期'].apply(lambda x: x.days)\n",
    "\n",
    "    df['货量'] = np.log(df['货量'])\n",
    "    # return df.drop(targetKey, axis=1), df[targetKey], df\n",
    "\n",
    "    grouped = df.groupby(['场地1', '场地2']).size().reset_index(name='count')\n",
    "    merged = pd.merge(df, grouped, on=['场地1', '场地2'], how='left').reset_index(drop=True)\n",
    "    ltSize = merged[merged['count'] < filterSize]\n",
    "    print(f'总行数有：{df.shape}')\n",
    "    print(f'数据小于{filterSize}的行数有：{ltSize.shape}')\n",
    "    filtered = merged[merged['count'] >= filterSize]\n",
    "    filtered = filtered.drop('count', axis=1)\n",
    "    print(f'清洗后数量：{filtered.shape}')\n",
    "    return filtered.drop(targetKey, axis=1), filtered[targetKey], filtered\n",
    "\n",
    "\n",
    "# 读入数据\n",
    "_, _, data = getxy()\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:22.212354Z",
     "end_time": "2023-04-16T14:55:24.338409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset数量: 1049\n",
      "trainX.shape: (732202, 32, 4)\n",
      "testX.shape: (73221, 32, 4)\n",
      "trainY.shape: (732202,)\n",
      "testY.shape: (73221,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将数据处理成模型可接受的形式\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        # a = dataset[i:(i + look_back), :-1]\n",
    "        # 自回归\n",
    "        # a = dataset[i:(i + look_back), [0, 1, 3]]\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, -1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "# 按照时间排序\n",
    "data = data.sort_values('日期')\n",
    "\n",
    "# 将每条有向边转换成一个序列\n",
    "dataset = []\n",
    "for _, group in data.groupby(['场地1', '场地2']):\n",
    "    dataset.append(group.values)\n",
    "\n",
    "print(f'dataset数量: {len(dataset)}')\n",
    "\n",
    "# 将序列转换成模型可接受的形式\n",
    "'''\n",
    "`look_back`是一个超参数，它定义了我们在创建时间序列数据集时要考虑多少个时间步。\n",
    "具体地说，对于每条有向边的货量时间序列，我们将数据集中的每个样本定义为过去`look_back`个时间步的货量，目标是预测下一个时间步的货量。\n",
    "\n",
    "例如，\n",
    "如果`look_back`设置为1，我们将使用过去1天的货量数据来预测下一天的货量。\n",
    "如果`look_back`设置为3，我们将使用过去3天的货量数据来预测下一天的货量。\n",
    "通过调整`look_back`，我们可以控制模型应该考虑多少历史数据来进行预测。\n",
    "'''\n",
    "look_back = filterSize - 1\n",
    "trainX, trainY = [], []\n",
    "for i in range(len(dataset)):\n",
    "    train_x, train_y = create_dataset(dataset[i], look_back)\n",
    "    trainX.append(train_x)\n",
    "    trainY.append(train_y)\n",
    "\n",
    "trainX, trainY = np.concatenate(trainX), np.concatenate(trainY)\n",
    "ratio = int(len(trainX) * 0.9)\n",
    "trainX, testX, trainY, testY = trainX[:], trainX[ratio:], trainY[:], trainY[ratio:]\n",
    "\n",
    "print(f'trainX.shape: {trainX.shape}')\n",
    "print(f'testX.shape: {testX.shape}')\n",
    "print(f'trainY.shape: {trainY.shape}')\n",
    "print(f'testY.shape: {testY.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:24.338409Z",
     "end_time": "2023-04-16T14:55:25.621409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.        ,  8.        ,  0.        ,  1.09861229],\n       [ 1.        ,  8.        ,  1.        ,  1.09861229],\n       [ 1.        ,  8.        ,  2.        ,  1.09861229],\n       [ 1.        ,  8.        ,  3.        ,  1.09861229],\n       [ 1.        ,  8.        ,  4.        ,  1.09861229],\n       [ 1.        ,  8.        ,  5.        ,  1.09861229],\n       [ 1.        ,  8.        ,  6.        ,  1.09861229],\n       [ 1.        ,  8.        ,  7.        ,  1.09861229],\n       [ 1.        ,  8.        ,  8.        ,  1.09861229],\n       [ 1.        ,  8.        ,  9.        ,  1.09861229],\n       [ 1.        ,  8.        , 10.        ,  1.09861229],\n       [ 1.        ,  8.        , 11.        ,  1.09861229],\n       [ 1.        ,  8.        , 12.        ,  1.09861229],\n       [ 1.        ,  8.        , 13.        ,  1.09861229],\n       [ 1.        ,  8.        , 14.        ,  1.09861229],\n       [ 1.        ,  8.        , 15.        ,  1.09861229],\n       [ 1.        ,  8.        , 16.        ,  1.09861229],\n       [ 1.        ,  8.        , 17.        ,  1.09861229],\n       [ 1.        ,  8.        , 18.        ,  1.09861229],\n       [ 1.        ,  8.        , 19.        ,  1.09861229],\n       [ 1.        ,  8.        , 20.        ,  1.09861229],\n       [ 1.        ,  8.        , 21.        ,  1.09861229],\n       [ 1.        ,  8.        , 22.        ,  1.09861229],\n       [ 1.        ,  8.        , 23.        ,  1.09861229],\n       [ 1.        ,  8.        , 24.        ,  1.09861229],\n       [ 1.        ,  8.        , 25.        ,  1.09861229],\n       [ 1.        ,  8.        , 26.        ,  1.09861229],\n       [ 1.        ,  8.        , 27.        ,  1.09861229],\n       [ 1.        ,  8.        , 28.        ,  1.09861229],\n       [ 1.        ,  8.        , 29.        ,  1.09861229],\n       [ 1.        ,  8.        , 30.        ,  1.09861229],\n       [ 1.        ,  8.        , 31.        ,  1.09861229]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:55:25.622408Z",
     "end_time": "2023-04-16T14:55:25.636409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "74/74 [==============================] - 6s 54ms/step - loss: 7.5140 - r_square: -0.0206 - val_loss: 2.6796 - val_r_square: -1.5447\n",
      "Epoch 2/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.9019 - r_square: 0.8776 - val_loss: 0.2445 - val_r_square: 0.9024\n",
      "Epoch 3/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.2973 - r_square: 0.9593 - val_loss: 0.2089 - val_r_square: 0.9226\n",
      "Epoch 4/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.2739 - r_square: 0.9626 - val_loss: 0.2051 - val_r_square: 0.9322\n",
      "Epoch 5/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.2669 - r_square: 0.9636 - val_loss: 0.2024 - val_r_square: 0.9256\n",
      "Epoch 6/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.2634 - r_square: 0.9640 - val_loss: 0.2246 - val_r_square: 0.9125\n",
      "Epoch 7/500\n",
      "74/74 [==============================] - 4s 50ms/step - loss: 0.2624 - r_square: 0.9642 - val_loss: 0.1985 - val_r_square: 0.9341\n",
      "Epoch 8/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.2611 - r_square: 0.9644 - val_loss: 0.1910 - val_r_square: 0.9353\n",
      "Epoch 9/500\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.2570 - r_square: 0.9649 - val_loss: 0.1883 - val_r_square: 0.9428\n",
      "Epoch 10/500\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.2562 - r_square: 0.9651 - val_loss: 0.1894 - val_r_square: 0.9432\n",
      "Epoch 11/500\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.2543 - r_square: 0.9653 - val_loss: 0.1930 - val_r_square: 0.9386\n",
      "Epoch 12/500\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.2560 - r_square: 0.9650 - val_loss: 0.2150 - val_r_square: 0.9036\n",
      "Epoch 13/500\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.2593 - r_square: 0.9646 - val_loss: 0.1942 - val_r_square: 0.9305\n",
      "Epoch 14/500\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.2519 - r_square: 0.9656 - val_loss: 0.2062 - val_r_square: 0.9263\n",
      "Epoch 15/500\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.2596 - r_square: 0.9646 - val_loss: 0.1959 - val_r_square: 0.9277\n",
      "Epoch 16/500\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.2518 - r_square: 0.9656 - val_loss: 0.1976 - val_r_square: 0.9221\n",
      "Epoch 17/500\n",
      "74/74 [==============================] - 4s 55ms/step - loss: 0.2508 - r_square: 0.9658 - val_loss: 0.1921 - val_r_square: 0.9395\n",
      "Epoch 18/500\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 0.2539 - r_square: 0.9654 - val_loss: 0.1869 - val_r_square: 0.9444\n",
      "Epoch 19/500\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 0.2505 - r_square: 0.9658 - val_loss: 0.1878 - val_r_square: 0.9401\n",
      "Epoch 20/500\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 0.2548 - r_square: 0.9652 - val_loss: 0.1938 - val_r_square: 0.9400\n",
      "Epoch 21/500\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 0.2487 - r_square: 0.9660 - val_loss: 0.1848 - val_r_square: 0.9446\n",
      "Epoch 22/500\n",
      "74/74 [==============================] - 4s 57ms/step - loss: 0.2488 - r_square: 0.9660 - val_loss: 0.1851 - val_r_square: 0.9442\n",
      "Epoch 23/500\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 0.2498 - r_square: 0.9659 - val_loss: 0.1876 - val_r_square: 0.9435\n",
      "Epoch 24/500\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 0.2492 - r_square: 0.9659 - val_loss: 0.2028 - val_r_square: 0.9352\n",
      "Epoch 25/500\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.2525 - r_square: 0.9656 - val_loss: 0.1866 - val_r_square: 0.9441\n",
      "Epoch 26/500\n",
      "74/74 [==============================] - 4s 58ms/step - loss: 0.2482 - r_square: 0.9661 - val_loss: 0.1843 - val_r_square: 0.9456\n",
      "Epoch 27/500\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.2477 - r_square: 0.9662 - val_loss: 0.1888 - val_r_square: 0.9421\n",
      "Epoch 28/500\n",
      "74/74 [==============================] - 4s 59ms/step - loss: 0.2478 - r_square: 0.9661 - val_loss: 0.1835 - val_r_square: 0.9453\n",
      "Epoch 29/500\n",
      "74/74 [==============================] - 4s 60ms/step - loss: 0.2490 - r_square: 0.9660 - val_loss: 0.1876 - val_r_square: 0.9430\n",
      "Epoch 30/500\n",
      "74/74 [==============================] - 5s 65ms/step - loss: 0.2521 - r_square: 0.9656 - val_loss: 0.1892 - val_r_square: 0.9371\n",
      "Epoch 31/500\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.2459 - r_square: 0.9665 - val_loss: 0.1869 - val_r_square: 0.9437\n",
      "Epoch 32/500\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.2466 - r_square: 0.9663 - val_loss: 0.1896 - val_r_square: 0.9345\n",
      "Epoch 33/500\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.2453 - r_square: 0.9665 - val_loss: 0.1838 - val_r_square: 0.9443\n",
      "Epoch 34/500\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.2460 - r_square: 0.9664 - val_loss: 0.1835 - val_r_square: 0.9448\n",
      "Epoch 35/500\n",
      "74/74 [==============================] - 5s 64ms/step - loss: 0.2478 - r_square: 0.9662 - val_loss: 0.1830 - val_r_square: 0.9458\n",
      "Epoch 36/500\n",
      "74/74 [==============================] - 5s 67ms/step - loss: 0.2453 - r_square: 0.9664 - val_loss: 0.1842 - val_r_square: 0.9423\n",
      "Epoch 37/500\n",
      "74/74 [==============================] - 5s 63ms/step - loss: 0.2443 - r_square: 0.9667 - val_loss: 0.1824 - val_r_square: 0.9468\n",
      "Epoch 38/500\n",
      "74/74 [==============================] - 5s 68ms/step - loss: 0.2436 - r_square: 0.9668 - val_loss: 0.1821 - val_r_square: 0.9463\n",
      "Epoch 39/500\n",
      "74/74 [==============================] - 5s 69ms/step - loss: 0.2445 - r_square: 0.9665 - val_loss: 0.1869 - val_r_square: 0.9426\n",
      "Epoch 40/500\n",
      "74/74 [==============================] - 5s 69ms/step - loss: 0.2455 - r_square: 0.9665 - val_loss: 0.1844 - val_r_square: 0.9455\n",
      "Epoch 41/500\n",
      "74/74 [==============================] - 5s 69ms/step - loss: 0.2439 - r_square: 0.9667 - val_loss: 0.1889 - val_r_square: 0.9407\n",
      "Epoch 42/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2465 - r_square: 0.9664 - val_loss: 0.1821 - val_r_square: 0.9468\n",
      "Epoch 43/500\n",
      "74/74 [==============================] - 5s 69ms/step - loss: 0.2461 - r_square: 0.9664 - val_loss: 0.1818 - val_r_square: 0.9468\n",
      "Epoch 44/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2435 - r_square: 0.9667 - val_loss: 0.1827 - val_r_square: 0.9442\n",
      "Epoch 45/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2434 - r_square: 0.9667 - val_loss: 0.1829 - val_r_square: 0.9441\n",
      "Epoch 46/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2439 - r_square: 0.9668 - val_loss: 0.1822 - val_r_square: 0.9466\n",
      "Epoch 47/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2416 - r_square: 0.9671 - val_loss: 0.1846 - val_r_square: 0.9431\n",
      "Epoch 48/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2418 - r_square: 0.9670 - val_loss: 0.1836 - val_r_square: 0.9454\n",
      "Epoch 49/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2429 - r_square: 0.9667 - val_loss: 0.1820 - val_r_square: 0.9459\n",
      "Epoch 50/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2471 - r_square: 0.9662 - val_loss: 0.2050 - val_r_square: 0.9160\n",
      "Epoch 51/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2415 - r_square: 0.9671 - val_loss: 0.1818 - val_r_square: 0.9448\n",
      "Epoch 52/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2408 - r_square: 0.9671 - val_loss: 0.1982 - val_r_square: 0.9343\n",
      "Epoch 53/500\n",
      "74/74 [==============================] - 5s 74ms/step - loss: 0.2426 - r_square: 0.9670 - val_loss: 0.1827 - val_r_square: 0.9460\n",
      "Epoch 54/500\n",
      "74/74 [==============================] - 5s 74ms/step - loss: 0.2439 - r_square: 0.9667 - val_loss: 0.1824 - val_r_square: 0.9436\n",
      "Epoch 55/500\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 0.2413 - r_square: 0.9670 - val_loss: 0.1857 - val_r_square: 0.9439\n",
      "Epoch 56/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2440 - r_square: 0.9667 - val_loss: 0.1847 - val_r_square: 0.9395\n",
      "Epoch 57/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2402 - r_square: 0.9672 - val_loss: 0.2006 - val_r_square: 0.9293\n",
      "Epoch 58/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2418 - r_square: 0.9670 - val_loss: 0.1823 - val_r_square: 0.9427\n",
      "Epoch 59/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2416 - r_square: 0.9670 - val_loss: 0.1855 - val_r_square: 0.9437\n",
      "Epoch 60/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2403 - r_square: 0.9672 - val_loss: 0.1819 - val_r_square: 0.9448\n",
      "Epoch 61/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2400 - r_square: 0.9673 - val_loss: 0.1800 - val_r_square: 0.9473\n",
      "Epoch 62/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2407 - r_square: 0.9671 - val_loss: 0.1838 - val_r_square: 0.9456\n",
      "Epoch 63/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2416 - r_square: 0.9670 - val_loss: 0.1810 - val_r_square: 0.9457\n",
      "Epoch 64/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2400 - r_square: 0.9672 - val_loss: 0.1802 - val_r_square: 0.9469\n",
      "Epoch 65/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2402 - r_square: 0.9672 - val_loss: 0.1822 - val_r_square: 0.9448\n",
      "Epoch 66/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2391 - r_square: 0.9674 - val_loss: 0.1821 - val_r_square: 0.9446\n",
      "Epoch 67/500\n",
      "74/74 [==============================] - 6s 75ms/step - loss: 0.2409 - r_square: 0.9672 - val_loss: 0.1819 - val_r_square: 0.9467\n",
      "Epoch 68/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2394 - r_square: 0.9674 - val_loss: 0.1848 - val_r_square: 0.9385\n",
      "Epoch 69/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2401 - r_square: 0.9672 - val_loss: 0.1790 - val_r_square: 0.9475\n",
      "Epoch 70/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2385 - r_square: 0.9674 - val_loss: 0.1797 - val_r_square: 0.9468\n",
      "Epoch 71/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2397 - r_square: 0.9673 - val_loss: 0.1800 - val_r_square: 0.9469\n",
      "Epoch 72/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2383 - r_square: 0.9675 - val_loss: 0.1818 - val_r_square: 0.9436\n",
      "Epoch 73/500\n",
      "74/74 [==============================] - 5s 70ms/step - loss: 0.2412 - r_square: 0.9671 - val_loss: 0.1802 - val_r_square: 0.9475\n",
      "Epoch 74/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2392 - r_square: 0.9674 - val_loss: 0.1820 - val_r_square: 0.9461\n",
      "Epoch 75/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2384 - r_square: 0.9674 - val_loss: 0.1803 - val_r_square: 0.9465\n",
      "Epoch 76/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2381 - r_square: 0.9676 - val_loss: 0.1876 - val_r_square: 0.9375\n",
      "Epoch 77/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2393 - r_square: 0.9674 - val_loss: 0.1799 - val_r_square: 0.9472\n",
      "Epoch 78/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2387 - r_square: 0.9674 - val_loss: 0.1799 - val_r_square: 0.9437\n",
      "Epoch 79/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2371 - r_square: 0.9676 - val_loss: 0.1792 - val_r_square: 0.9464\n",
      "Epoch 80/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2377 - r_square: 0.9675 - val_loss: 0.1811 - val_r_square: 0.9459\n",
      "Epoch 81/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2385 - r_square: 0.9674 - val_loss: 0.1816 - val_r_square: 0.9414\n",
      "Epoch 82/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2396 - r_square: 0.9674 - val_loss: 0.1833 - val_r_square: 0.9419\n",
      "Epoch 83/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2373 - r_square: 0.9676 - val_loss: 0.1789 - val_r_square: 0.9455\n",
      "Epoch 84/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2385 - r_square: 0.9675 - val_loss: 0.1842 - val_r_square: 0.9419\n",
      "Epoch 85/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2383 - r_square: 0.9674 - val_loss: 0.1786 - val_r_square: 0.9475\n",
      "Epoch 86/500\n",
      "74/74 [==============================] - 5s 72ms/step - loss: 0.2368 - r_square: 0.9677 - val_loss: 0.1797 - val_r_square: 0.9463\n",
      "Epoch 87/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2408 - r_square: 0.9671 - val_loss: 0.1789 - val_r_square: 0.9467\n",
      "Epoch 88/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2373 - r_square: 0.9676 - val_loss: 0.1787 - val_r_square: 0.9477\n",
      "Epoch 89/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2365 - r_square: 0.9676 - val_loss: 0.1833 - val_r_square: 0.9408\n",
      "Epoch 90/500\n",
      "74/74 [==============================] - 5s 71ms/step - loss: 0.2397 - r_square: 0.9673 - val_loss: 0.1799 - val_r_square: 0.9447\n",
      "Epoch 91/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2361 - r_square: 0.9678 - val_loss: 0.1820 - val_r_square: 0.9439\n",
      "Epoch 92/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2363 - r_square: 0.9677 - val_loss: 0.1830 - val_r_square: 0.9446\n",
      "Epoch 93/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2397 - r_square: 0.9673 - val_loss: 0.1776 - val_r_square: 0.9481\n",
      "Epoch 94/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2372 - r_square: 0.9676 - val_loss: 0.1801 - val_r_square: 0.9435\n",
      "Epoch 95/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2363 - r_square: 0.9677 - val_loss: 0.1835 - val_r_square: 0.9388\n",
      "Epoch 96/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2373 - r_square: 0.9676 - val_loss: 0.1852 - val_r_square: 0.9438\n",
      "Epoch 97/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2372 - r_square: 0.9676 - val_loss: 0.1802 - val_r_square: 0.9403\n",
      "Epoch 98/500\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.2366 - r_square: 0.9677 - val_loss: 0.1864 - val_r_square: 0.9390\n",
      "Epoch 99/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2361 - r_square: 0.9677 - val_loss: 0.1862 - val_r_square: 0.9386\n",
      "Epoch 100/500\n",
      "74/74 [==============================] - 6s 87ms/step - loss: 0.2351 - r_square: 0.9679 - val_loss: 0.1798 - val_r_square: 0.9471\n",
      "Epoch 101/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2367 - r_square: 0.9677 - val_loss: 0.1782 - val_r_square: 0.9474\n",
      "Epoch 102/500\n",
      "74/74 [==============================] - 5s 73ms/step - loss: 0.2350 - r_square: 0.9679 - val_loss: 0.1833 - val_r_square: 0.9338\n",
      "Epoch 103/500\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.2355 - r_square: 0.9679 - val_loss: 0.1776 - val_r_square: 0.9454\n",
      "Epoch 104/500\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.2356 - r_square: 0.9678 - val_loss: 0.1773 - val_r_square: 0.9465\n",
      "Epoch 105/500\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.2373 - r_square: 0.9676 - val_loss: 0.1810 - val_r_square: 0.9434\n",
      "Epoch 106/500\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 0.2354 - r_square: 0.9679 - val_loss: 0.1812 - val_r_square: 0.9431\n",
      "Epoch 107/500\n",
      "74/74 [==============================] - 6s 87ms/step - loss: 0.2377 - r_square: 0.9676 - val_loss: 0.1790 - val_r_square: 0.9443\n",
      "Epoch 108/500\n",
      "74/74 [==============================] - 5s 74ms/step - loss: 0.2350 - r_square: 0.9679 - val_loss: 0.1778 - val_r_square: 0.9478\n",
      "Epoch 109/500\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.2341 - r_square: 0.9681 - val_loss: 0.1800 - val_r_square: 0.9442\n",
      "Epoch 110/500\n",
      "74/74 [==============================] - 6s 82ms/step - loss: 0.2348 - r_square: 0.9680 - val_loss: 0.1806 - val_r_square: 0.9459\n",
      "Epoch 111/500\n",
      "74/74 [==============================] - 7s 93ms/step - loss: 0.2366 - r_square: 0.9678 - val_loss: 0.1831 - val_r_square: 0.9376\n",
      "Epoch 112/500\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 0.2350 - r_square: 0.9680 - val_loss: 0.1765 - val_r_square: 0.9475\n",
      "Epoch 113/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2372 - r_square: 0.9675 - val_loss: 0.1762 - val_r_square: 0.9483\n",
      "Epoch 114/500\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.2347 - r_square: 0.9680 - val_loss: 0.1798 - val_r_square: 0.9447\n",
      "Epoch 115/500\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 0.2350 - r_square: 0.9679 - val_loss: 0.1808 - val_r_square: 0.9379\n",
      "Epoch 116/500\n",
      "74/74 [==============================] - 7s 94ms/step - loss: 0.2343 - r_square: 0.9680 - val_loss: 0.1853 - val_r_square: 0.9380\n",
      "Epoch 117/500\n",
      "74/74 [==============================] - 7s 93ms/step - loss: 0.2342 - r_square: 0.9681 - val_loss: 0.1834 - val_r_square: 0.9408\n",
      "Epoch 118/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2347 - r_square: 0.9680 - val_loss: 0.1794 - val_r_square: 0.9468\n",
      "Epoch 119/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2357 - r_square: 0.9678 - val_loss: 0.1853 - val_r_square: 0.9391\n",
      "Epoch 120/500\n",
      "74/74 [==============================] - 6s 84ms/step - loss: 0.2351 - r_square: 0.9679 - val_loss: 0.1782 - val_r_square: 0.9466\n",
      "Epoch 121/500\n",
      "74/74 [==============================] - 7s 90ms/step - loss: 0.2349 - r_square: 0.9680 - val_loss: 0.1776 - val_r_square: 0.9471\n",
      "Epoch 122/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2359 - r_square: 0.9678 - val_loss: 0.1802 - val_r_square: 0.9450\n",
      "Epoch 123/500\n",
      "74/74 [==============================] - 7s 90ms/step - loss: 0.2357 - r_square: 0.9678 - val_loss: 0.1781 - val_r_square: 0.9478\n",
      "Epoch 124/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2342 - r_square: 0.9680 - val_loss: 0.1835 - val_r_square: 0.9357\n",
      "Epoch 125/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2350 - r_square: 0.9679 - val_loss: 0.1773 - val_r_square: 0.9473\n",
      "Epoch 126/500\n",
      "74/74 [==============================] - 7s 90ms/step - loss: 0.2337 - r_square: 0.9681 - val_loss: 0.1793 - val_r_square: 0.9469\n",
      "Epoch 127/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2332 - r_square: 0.9682 - val_loss: 0.1777 - val_r_square: 0.9478\n",
      "Epoch 128/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2354 - r_square: 0.9678 - val_loss: 0.1759 - val_r_square: 0.9481\n",
      "Epoch 129/500\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.2347 - r_square: 0.9679 - val_loss: 0.1768 - val_r_square: 0.9467\n",
      "Epoch 130/500\n",
      "74/74 [==============================] - 6s 87ms/step - loss: 0.2328 - r_square: 0.9682 - val_loss: 0.1777 - val_r_square: 0.9478\n",
      "Epoch 131/500\n",
      "74/74 [==============================] - 7s 93ms/step - loss: 0.2334 - r_square: 0.9681 - val_loss: 0.1829 - val_r_square: 0.9386\n",
      "Epoch 132/500\n",
      "74/74 [==============================] - 7s 93ms/step - loss: 0.2330 - r_square: 0.9682 - val_loss: 0.1860 - val_r_square: 0.9313\n",
      "Epoch 133/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2354 - r_square: 0.9678 - val_loss: 0.1852 - val_r_square: 0.9399\n",
      "Epoch 134/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2335 - r_square: 0.9680 - val_loss: 0.1777 - val_r_square: 0.9436\n",
      "Epoch 135/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2353 - r_square: 0.9679 - val_loss: 0.1778 - val_r_square: 0.9476\n",
      "Epoch 136/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2324 - r_square: 0.9683 - val_loss: 0.1764 - val_r_square: 0.9465\n",
      "Epoch 137/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2326 - r_square: 0.9683 - val_loss: 0.1780 - val_r_square: 0.9433\n",
      "Epoch 138/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2322 - r_square: 0.9682 - val_loss: 0.1762 - val_r_square: 0.9483\n",
      "Epoch 139/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2332 - r_square: 0.9682 - val_loss: 0.1766 - val_r_square: 0.9478\n",
      "Epoch 140/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2324 - r_square: 0.9683 - val_loss: 0.1755 - val_r_square: 0.9483\n",
      "Epoch 141/500\n",
      "74/74 [==============================] - 6s 84ms/step - loss: 0.2311 - r_square: 0.9684 - val_loss: 0.1797 - val_r_square: 0.9455\n",
      "Epoch 142/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2328 - r_square: 0.9682 - val_loss: 0.1757 - val_r_square: 0.9486\n",
      "Epoch 143/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2319 - r_square: 0.9683 - val_loss: 0.1841 - val_r_square: 0.9342\n",
      "Epoch 144/500\n",
      "74/74 [==============================] - 7s 90ms/step - loss: 0.2329 - r_square: 0.9683 - val_loss: 0.1815 - val_r_square: 0.9433\n",
      "Epoch 145/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2322 - r_square: 0.9683 - val_loss: 0.1745 - val_r_square: 0.9489\n",
      "Epoch 146/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2324 - r_square: 0.9683 - val_loss: 0.1753 - val_r_square: 0.9473\n",
      "Epoch 147/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2305 - r_square: 0.9684 - val_loss: 0.1768 - val_r_square: 0.9475\n",
      "Epoch 148/500\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.2306 - r_square: 0.9685 - val_loss: 0.1773 - val_r_square: 0.9436\n",
      "Epoch 149/500\n",
      "74/74 [==============================] - 6s 84ms/step - loss: 0.2306 - r_square: 0.9686 - val_loss: 0.1770 - val_r_square: 0.9450\n",
      "Epoch 150/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2319 - r_square: 0.9684 - val_loss: 0.1752 - val_r_square: 0.9477\n",
      "Epoch 151/500\n",
      "74/74 [==============================] - 7s 93ms/step - loss: 0.2323 - r_square: 0.9682 - val_loss: 0.1752 - val_r_square: 0.9489\n",
      "Epoch 152/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2307 - r_square: 0.9686 - val_loss: 0.1744 - val_r_square: 0.9487\n",
      "Epoch 153/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2302 - r_square: 0.9686 - val_loss: 0.1793 - val_r_square: 0.9397\n",
      "Epoch 154/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2306 - r_square: 0.9686 - val_loss: 0.1752 - val_r_square: 0.9486\n",
      "Epoch 155/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2312 - r_square: 0.9684 - val_loss: 0.1847 - val_r_square: 0.9370\n",
      "Epoch 156/500\n",
      "74/74 [==============================] - 6s 82ms/step - loss: 0.2323 - r_square: 0.9683 - val_loss: 0.1749 - val_r_square: 0.9492\n",
      "Epoch 157/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2316 - r_square: 0.9684 - val_loss: 0.1743 - val_r_square: 0.9475\n",
      "Epoch 158/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2303 - r_square: 0.9686 - val_loss: 0.1755 - val_r_square: 0.9485\n",
      "Epoch 159/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2328 - r_square: 0.9682 - val_loss: 0.1746 - val_r_square: 0.9484\n",
      "Epoch 160/500\n",
      "74/74 [==============================] - 6s 86ms/step - loss: 0.2302 - r_square: 0.9686 - val_loss: 0.1760 - val_r_square: 0.9475\n",
      "Epoch 161/500\n",
      "74/74 [==============================] - 6s 85ms/step - loss: 0.2298 - r_square: 0.9686 - val_loss: 0.1772 - val_r_square: 0.9485\n",
      "Epoch 162/500\n",
      "74/74 [==============================] - 6s 86ms/step - loss: 0.2300 - r_square: 0.9685 - val_loss: 0.1797 - val_r_square: 0.9423\n",
      "Epoch 163/500\n",
      "74/74 [==============================] - 6s 87ms/step - loss: 0.2301 - r_square: 0.9686 - val_loss: 0.1768 - val_r_square: 0.9468\n",
      "Epoch 164/500\n",
      "74/74 [==============================] - 6s 87ms/step - loss: 0.2295 - r_square: 0.9686 - val_loss: 0.1763 - val_r_square: 0.9482\n",
      "Epoch 165/500\n",
      "74/74 [==============================] - 7s 100ms/step - loss: 0.2298 - r_square: 0.9686 - val_loss: 0.1767 - val_r_square: 0.9466\n",
      "Epoch 166/500\n",
      "74/74 [==============================] - 7s 92ms/step - loss: 0.2294 - r_square: 0.9686 - val_loss: 0.1735 - val_r_square: 0.9489\n",
      "Epoch 167/500\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.2286 - r_square: 0.9688 - val_loss: 0.1759 - val_r_square: 0.9472\n",
      "Epoch 168/500\n",
      "24/74 [========>.....................] - ETA: 4s - loss: 0.2293 - r_square: 0.9686"
     ]
    }
   ],
   "source": [
    "\n",
    "modelPath = 'LSTM.33.h5'\n",
    "\n",
    "\n",
    "# 定义自定义指标函数\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res = tf.keras.backend.sum(tf.keras.backend.square(y_true - y_pred))\n",
    "    SS_tot = tf.keras.backend.sum(tf.keras.backend.square(y_true - tf.keras.backend.mean(y_true)))\n",
    "    return (1 - SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "if not os.path.exists(modelPath):\n",
    "    # 定义和训练LSTM模型\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, input_shape=(look_back, trainX.shape[2])),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    metrics = r_square\n",
    "    metricsName = metrics.__name__\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[metrics])\n",
    "    # epochs=3000, batch_size=5000\n",
    "    history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=500, batch_size=10000)\n",
    "    model.save(modelPath)\n",
    "\n",
    "    # 绘制学习曲线\n",
    "    loss = pd.DataFrame({'训练集损失': history.history['loss'], '测试集损失': history.history['val_loss'], })\n",
    "    acc = pd.DataFrame(\n",
    "        {'训练集准确率': history.history[metricsName], '测试集准确率': history.history[f'val_{metricsName}'], })\n",
    "\n",
    "    sns.lineplot(loss)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE')\n",
    "    # 绘制最后一个点并添加横线\n",
    "    last_loss = history.history['loss'][-1]\n",
    "    plt.plot(len(history.history['loss']) - 1, last_loss, marker='o', color='darkred')\n",
    "    plt.axhline(y=last_loss, color='darkred', linestyle='--')\n",
    "    # 添加红色直线的 y 值标签\n",
    "    plt.text(len(loss) - 15, last_loss + 1, f'训练集y={last_loss:.4f}', color='darkred')\n",
    "    # 绘制最后一个点并添加横线\n",
    "    last_loss = history.history['val_loss'][-1]\n",
    "    plt.plot(len(history.history['val_loss']) - 1, last_loss, marker='o', color='darkred')\n",
    "    plt.axhline(y=last_loss, color='darkred', linestyle='--')\n",
    "    # 添加红色直线的 y 值标签\n",
    "    plt.text(len(loss) - 15, last_loss + 5, f'测试集y={last_loss:.4f}', color='darkred')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('损失率随训练代数的变化.svg')\n",
    "    plt.show()\n",
    "\n",
    "    sns.lineplot(acc)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('R2')\n",
    "    # 绘制最后一个点并添加横线\n",
    "    last_acc = history.history[metricsName][-1]\n",
    "    plt.plot(len(history.history[metricsName]) - 1, last_acc, marker='o', color='darkred')\n",
    "    plt.axhline(y=last_acc, color='darkred', linestyle='--')\n",
    "    # 添加红色直线的 y 值标签\n",
    "    plt.text(len(acc) - 10, last_acc + 0.1, f'训练集y={last_acc:.4f}', color='darkred')\n",
    "    # 绘制最后一个点并添加横线\n",
    "    last_acc = history.history[f'val_{metricsName}'][-1]\n",
    "    plt.plot(len(history.history[f'val_{metricsName}']) - 1, last_acc, marker='o', color='darkred')\n",
    "    plt.axhline(y=last_acc, color='darkred', linestyle='--')\n",
    "    # 添加红色直线的 y 值标签\n",
    "    plt.text(len(acc) - 10, last_acc - 0.1, f'测试集y={last_acc:.4f}', color='darkred')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('准确率随训练代数的变化.svg')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # 加载模型并继续训练\n",
    "    model = tf.keras.models.load_model(modelPath, custom_objects={'r_square': r_square})\n",
    "\n",
    "# 测试模型\n",
    "y_pred = model.predict(trainX, batch_size=10000)\n",
    "\n",
    "# 绘制预测结果和真实值的比较图\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.plot(np.exp(trainY), label='真实值')\n",
    "plt.plot(np.exp(y_pred), label='预测值')\n",
    "plt.ylabel('货量')\n",
    "plt.xlabel('数据序号')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('绘制预测结果和真实值的比较图.svg')\n",
    "plt.show()\n",
    "\n",
    "# y预测值-x真实值\n",
    "sns.scatterplot({'预测值': np.exp(trainY), '真实值': np.exp(y_pred.reshape(len(y_pred)))}, x='真实值', y='预测值')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "_ = plt.plot([-1e10, 1e10], [-1e10, 1e10])\n",
    "plt.tight_layout()\n",
    "plt.savefig('y预测值-x真实值.svg')\n",
    "plt.savefig('y预测值-x真实值.png')\n",
    "plt.show()\n",
    "\n",
    "print('r2', r2_score(np.exp(trainY), np.exp(y_pred)))\n",
    "print('mse', mean_squared_error(np.exp(trainY), np.exp(y_pred)))\n",
    "print('mae', mean_absolute_error(np.exp(trainY), np.exp(y_pred)))\n",
    "print('mape', mean_absolute_percentage_error(np.exp(trainY), np.exp(y_pred)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:13:51.690268Z",
     "end_time": "2023-04-16T14:19:02.011392Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "敏感性分析\n",
    "'''\n",
    "# explainerData = np.copy(trainX)\n",
    "# np.random.shuffle(trainX)\n",
    "# explainerData = explainerData[:1000]\n",
    "# explainer = shap.DeepExplainer(model, explainerData)\n",
    "# shap_values = explainer.shap_values(explainerData)\n",
    "# #\n",
    "# # # plt.figure(figsize=(8, 6))\n",
    "# # # shap.plots.waterfall(shap_values[0][:, 0, :], show=False)\n",
    "# # # plt.tight_layout()\n",
    "# # # plt.savefig('灵敏度瀑布图.svg')\n",
    "# # # plt.show()\n",
    "# # #\n",
    "# # # plt.figure(figsize=(8, 6))\n",
    "# # # shap.plots.bar(shap_values, show=False)\n",
    "# # # plt.tight_layout()\n",
    "# # # plt.savefig('灵敏度条形图.svg')\n",
    "# # # plt.show()\n",
    "# #\n",
    "# #\n",
    "# plt.figure(figsize=(16, 9))\n",
    "# shap.summary_plot(shap_values[0][:, 0, :], explainerData[:][:, 0, :], data.columns[[0, 1, 3]], show=False, )\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('灵敏度蜂群图.svg')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-16T14:19:02.011392Z",
     "end_time": "2023-04-16T14:19:02.025218Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
